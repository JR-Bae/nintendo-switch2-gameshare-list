#!/usr/bin/env python3
"""
ë‹Œí…ë„ ìŠ¤ìœ„ì¹˜ 2 ë‚˜ëˆ”í†µì‹ (GameShare) ì§€ì› ê²Œì„ ëª©ë¡ - ë‹¤ì§€ì—­ ìŠ¤í¬ë ˆì´í¼
Nintendo Switch 2 GameShare Supported Games - Multi-Region Scraper

Korea (KR): requests + BeautifulSoup (Magento HTML)
US        : Playwright (Next.js / React)
Japan (JP): Playwright (SFCC / React)

ì¶œë ¥: gameshare_multi_YYYYMMDD_HHMMSS.csv
"""

import csv
import json
import re
import time
import unicodedata
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# â”€â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
}

KR_BASE   = "https://store.nintendo.co.kr"
KR_LIST   = f"{KR_BASE}/all-product?label_platform=4679"
KR_DELAY  = 1.5

US_BASE   = "https://www.nintendo.com"
US_GAMESHARE_LOCAL_URL = (
    f"{US_BASE}/us/search/"
    "#cat=gme"
    "&f=waysToPlayLabels%2CcorePlatforms"
    "&waysToPlayLabels=GameShare%20with%20local%20users"
    "&corePlatforms=Nintendo%20Switch%202"
)
US_GAMESHARE_CHAT_URL = (
    f"{US_BASE}/us/search/"
    "#cat=gme"
    "&f=waysToPlayLabels%2CcorePlatforms"
    "&waysToPlayLabels=GameShare%20over%20GameChat"
    "&corePlatforms=Nintendo%20Switch%202"
)

JP_BASE = "https://store-jp.nintendo.com"
JP_LOCAL_URL = f"{JP_BASE}/list/tag/0078?labelPlatform=BEE"   # GameShare Local
JP_CHAT_URL  = f"{JP_BASE}/list/tag/0079?labelPlatform=BEE"   # GameShare GameChat

_DATE      = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_FILE = f"gameshare_multi_{_DATE}.csv"


# â”€â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def normalize(name: str) -> str:
    """ì´ë¦„ ì •ê·œí™” (ì†Œë¬¸ì, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°) - ì§€ì—­ ê°„ ë§¤ì¹­ìš©"""
    name = unicodedata.normalize("NFKC", name).lower()
    name = re.sub(r"[â„¢Â®Â©â„—â„ :ã€Œã€ã€ã€ã€ã€‘\-â€“â€”_.,!?'\"\s]+", " ", name).strip()
    return name


def scroll_to_bottom(page, pause=2.5, max_rounds=20):
    """ë¬´í•œ ìŠ¤í¬ë¡¤ í˜ì´ì§€ì—ì„œ ëª¨ë“  í•­ëª© ë¡œë“œ"""
    prev = 0
    for _ in range(max_rounds):
        page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        page.wait_for_timeout(int(pause * 1000))
        cur = page.evaluate("document.body.scrollHeight")
        if cur == prev:
            break
        prev = cur


# â”€â”€â”€ KR ìŠ¤í¬ë ˆì´í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def kr_get_all_urls() -> list[str]:
    """í•œêµ­ ìŠ¤í† ì–´ Switch 2 ì „ì²´ ê²Œì„ URL ìˆ˜ì§‘"""
    urls: list[str] = []
    page_num = 1
    print("[KR] URL ìˆ˜ì§‘ ì‹œì‘...")
    while True:
        r = requests.get(
            f"{KR_LIST}&p={page_num}",
            headers={**HEADERS, "Accept-Language": "ko-KR,ko;q=0.9"},
            timeout=20,
        )
        if r.status_code != 200:
            break
        soup = BeautifulSoup(r.text, "lxml")
        items = soup.select("li.product-item")
        if not items:
            break
        for item in items:
            a = item.select_one("a.product-item-link")
            if a and a.get("href"):
                urls.append(a["href"].strip())
        total_raw = soup.select_one(".toolbar-amount")
        print(f"  í˜ì´ì§€ {page_num} ì™„ë£Œ: ëˆ„ì  {len(urls)}ê°œ", end="")
        if total_raw:
            print(f" (ì´ {total_raw.get_text(strip=True)})", end="")
        print()
        page_num += 1
        time.sleep(KR_DELAY)
    return urls


def kr_check_game(url: str, session: requests.Session) -> dict | None:
    """ê°œë³„ ê²Œì„ í˜ì´ì§€ì—ì„œ GameShare ì§€ì› ì—¬ë¶€ í™•ì¸"""
    for attempt in range(3):
        try:
            r = session.get(url, timeout=15)
            if r.status_code == 403:
                time.sleep(5)
                continue
            if r.status_code != 200:
                return None
            soup = BeautifulSoup(r.text, "lxml")

            def has_gameshare(css_class):
                div = soup.select_one(f"div.product-attribute.{css_class}")
                if not div:
                    return False
                val = div.select_one("div.attribute-item-val")
                return bool(val and "ëŒ€ì‘" in val.get_text())

            local = has_gameshare("supported_game_share_l")
            chat  = has_gameshare("supported_game_share_c")

            if not (local or chat):
                return None

            title = soup.select_one("h1.page-title span")
            name  = title.get_text(strip=True) if title else ""

            return {
                "name": name,
                "kr_url": url,
                "kr_local": local,
                "kr_chat": chat,
            }
        except Exception:
            time.sleep(3)
    return None


def scrape_kr() -> list[dict]:
    print("\nâ•â•â•â•â•â•â•â•â•â• ğŸ‡°ğŸ‡· í•œêµ­ ìŠ¤í† ì–´ ìŠ¤í¬ë© â•â•â•â•â•â•â•â•â•â•")
    session = requests.Session()
    session.headers.update({**HEADERS, "Accept-Language": "ko-KR,ko;q=0.9"})

    all_urls = kr_get_all_urls()
    results  = []
    print(f"[KR] ì´ {len(all_urls)}ê°œ URL ìƒì„¸ ìŠ¤ìº” ì‹œì‘...\n")

    for i, url in enumerate(all_urls, 1):
        data = kr_check_game(url, session)
        if data:
            results.append(data)
            marker = "â˜… GameShare"
            print(f"  [{i:3d}] {marker} | {data['name']}")
        else:
            print(f"  [{i:3d}]   -")
        time.sleep(KR_DELAY)

    print(f"\n[KR] ë‚˜ëˆ”í†µì‹  ì§€ì› ê²Œì„: {len(results)}ê°œ")
    return results


# â”€â”€â”€ US ìŠ¤í¬ë ˆì´í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def us_get_gameshare_slugs(page) -> tuple[set[str], set[str]]:
    """US ê²€ìƒ‰ ê²°ê³¼ì—ì„œ GameShare ê²Œì„ slug ìˆ˜ì§‘"""

    def load_slugs(url: str) -> set[str]:
        page.goto(url, wait_until="networkidle", timeout=40000)
        page.wait_for_timeout(3000)
        scroll_to_bottom(page)
        links = page.query_selector_all("a[href*='/us/store/products/']")
        slugs = set()
        for l in links:
            href = l.get_attribute("href") or ""
            m = re.search(r"/us/store/products/([^/?#]+)", href)
            if m:
                slugs.add(m.group(1))
        return slugs

    print("[US] GameShare Local ëª©ë¡ ìˆ˜ì§‘...")
    local_slugs = load_slugs(US_GAMESHARE_LOCAL_URL)
    print(f"  â†’ {len(local_slugs)}ê°œ")

    print("[US] GameShare Chat ëª©ë¡ ìˆ˜ì§‘...")
    chat_slugs = load_slugs(US_GAMESHARE_CHAT_URL)
    print(f"  â†’ {len(chat_slugs)}ê°œ")

    return local_slugs, chat_slugs


def us_get_game_name(page, slug: str) -> str:
    """ê°œë³„ US ê²Œì„ í˜ì´ì§€ì—ì„œ ê²Œì„ ì´ë¦„ ì¶”ì¶œ"""
    try:
        page.goto(
            f"{US_BASE}/us/store/products/{slug}/",
            wait_until="domcontentloaded",
            timeout=20000,
        )
        page.wait_for_timeout(1500)
        el = page.query_selector("h1")
        return el.inner_text().strip() if el else slug
    except Exception:
        return slug


def scrape_us(playwright) -> list[dict]:
    print("\nâ•â•â•â•â•â•â•â•â•â• ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ìŠ¤í† ì–´ ìŠ¤í¬ë© â•â•â•â•â•â•â•â•â•â•")
    browser = playwright.chromium.launch(headless=True)
    ctx  = browser.new_context(
        user_agent=HEADERS["User-Agent"], locale="en-US"
    )
    page = ctx.new_page()

    local_slugs, chat_slugs = us_get_gameshare_slugs(page)
    all_slugs = local_slugs | chat_slugs

    results = []
    print(f"[US] ì´ {len(all_slugs)}ê°œ GameShare ê²Œì„ ì´ë¦„ í™•ì¸...")
    for slug in sorted(all_slugs):
        name = us_get_game_name(page, slug)
        entry = {
            "name": name,
            "us_url": f"{US_BASE}/us/store/products/{slug}/",
            "us_local": slug in local_slugs,
            "us_chat":  slug in chat_slugs,
        }
        results.append(entry)
        icon = "â˜…"
        print(f"  {icon} {name}")
        time.sleep(0.5)

    browser.close()
    print(f"[US] GameShare ê²Œì„: {len(results)}ê°œ")
    return results


# â”€â”€â”€ JP ìŠ¤í¬ë ˆì´í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def jp_get_gameshare_ids(page, url: str, label: str) -> set[str]:
    """JP ìŠ¤í† ì–´ íƒœê·¸ í˜ì´ì§€ì—ì„œ ê²Œì„ ID ìˆ˜ì§‘ (í˜ì´ì§€ ìŠ¤í¬ë¡¤)"""
    print(f"[JP] {label} ëª©ë¡ ìˆ˜ì§‘...")
    page.goto(url, wait_until="domcontentloaded", timeout=60000)
    page.wait_for_timeout(4000)
    scroll_to_bottom(page, pause=2.0)

    links = page.query_selector_all("a[href*='/item/software/D']")
    ids = set()
    for l in links:
        href = l.get_attribute("href") or ""
        m = re.search(r"/item/software/(D\w+)", href)
        if m:
            ids.add(m.group(1))
    print(f"  â†’ {len(ids)}ê°œ")
    return ids


def jp_get_game_name(page, product_id: str) -> str:
    """JP ê²Œì„ í˜ì´ì§€ì—ì„œ ì´ë¦„ ì¶”ì¶œ"""
    try:
        page.goto(
            f"{JP_BASE}/item/software/{product_id}",
            wait_until="domcontentloaded",
            timeout=30000,
        )
        page.wait_for_timeout(3000)

        state_raw = page.evaluate("""() => {
            if (window.__PRELOADED_STATE__) return JSON.stringify(window.__PRELOADED_STATE__);
            return null;
        }""")
        if state_raw:
            state = json.loads(state_raw)
            queries = state.get("__reactQuery", {}).get("queries", [])
            for q in queries:
                d = q.get("state", {}).get("data", {})
                if isinstance(d, dict) and d.get("title"):
                    return d["title"]
                if isinstance(d, dict) and d.get("name"):
                    return d["name"]

        el = page.query_selector("h1")
        return el.inner_text().strip() if el else product_id
    except Exception:
        return product_id


def scrape_jp(playwright) -> list[dict]:
    print("\nâ•â•â•â•â•â•â•â•â•â• ğŸ‡¯ğŸ‡µ ì¼ë³¸ ìŠ¤í† ì–´ ìŠ¤í¬ë© â•â•â•â•â•â•â•â•â•â•")
    browser = playwright.chromium.launch(headless=True)
    ctx  = browser.new_context(
        user_agent=HEADERS["User-Agent"], locale="ja-JP"
    )
    page = ctx.new_page()

    local_ids = jp_get_gameshare_ids(page, JP_LOCAL_URL, "GameShare Local")
    chat_ids  = jp_get_gameshare_ids(page, JP_CHAT_URL,  "GameShare Chat")
    all_ids   = local_ids | chat_ids

    results = []
    print(f"[JP] ì´ {len(all_ids)}ê°œ GameShare ê²Œì„ ì´ë¦„ í™•ì¸...")
    for pid in sorted(all_ids):
        name = jp_get_game_name(page, pid)
        entry = {
            "name": name,
            "jp_url": f"{JP_BASE}/item/software/{pid}",
            "jp_local": pid in local_ids,
            "jp_chat":  pid in chat_ids,
        }
        results.append(entry)
        print(f"  â˜… {name}")
        time.sleep(0.5)

    browser.close()
    print(f"[JP] GameShare ê²Œì„: {len(results)}ê°œ")
    return results


# â”€â”€â”€ ì§€ì—­ ê°„ ë§¤ì¹­ & CSV ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def merge_results(kr_list, us_list, jp_list) -> list[dict]:
    """ê²Œì„ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì—­ë³„ ê²°ê³¼ ë³‘í•©"""

    def make_lookup(lst, name_key="name"):
        return {normalize(g[name_key]): g for g in lst}

    kr_map = make_lookup(kr_list)
    us_map = make_lookup(us_list)
    jp_map = make_lookup(jp_list)

    all_names: dict[str, dict] = {}  # norm_name -> merged row

    for norm, g in kr_map.items():
        all_names[norm] = {
            "name_kr": g["name"],
            "name_us": "",
            "name_jp": "",
            "kr_local": g.get("kr_local", False),
            "kr_chat":  g.get("kr_chat",  False),
            "us_local": False,
            "us_chat":  False,
            "jp_local": False,
            "jp_chat":  False,
            "kr_url": g.get("kr_url", ""),
            "us_url": "",
            "jp_url": "",
        }

    for norm, g in us_map.items():
        best = _fuzzy_match(norm, all_names)
        if best:
            all_names[best]["name_us"] = g["name"]
            all_names[best]["us_local"] = g.get("us_local", False)
            all_names[best]["us_chat"]  = g.get("us_chat",  False)
            all_names[best]["us_url"]   = g.get("us_url", "")
        else:
            all_names[norm] = {
                "name_kr": "",
                "name_us": g["name"],
                "name_jp": "",
                "kr_local": False, "kr_chat": False,
                "us_local": g.get("us_local", False),
                "us_chat":  g.get("us_chat",  False),
                "jp_local": False, "jp_chat": False,
                "kr_url": "",
                "us_url": g.get("us_url", ""),
                "jp_url": "",
            }

    for norm, g in jp_map.items():
        best = _fuzzy_match(norm, all_names)
        if best:
            all_names[best]["name_jp"] = g["name"]
            all_names[best]["jp_local"] = g.get("jp_local", False)
            all_names[best]["jp_chat"]  = g.get("jp_chat",  False)
            all_names[best]["jp_url"]   = g.get("jp_url", "")
        else:
            all_names[norm] = {
                "name_kr": "",
                "name_us": "",
                "name_jp": g["name"],
                "kr_local": False, "kr_chat": False,
                "us_local": False, "us_chat": False,
                "jp_local": g.get("jp_local", False),
                "jp_chat":  g.get("jp_chat",  False),
                "kr_url": "",
                "us_url": "",
                "jp_url": g.get("jp_url", ""),
            }

    rows = list(all_names.values())
    # ì´ë¦„ ì •ë ¬ (KR â†’ US â†’ JP ìˆœ)
    rows.sort(key=lambda r: (r["name_kr"] or r["name_us"] or r["name_jp"]).lower())
    return rows


def _fuzzy_match(norm: str, pool: dict, threshold=0.6) -> str | None:
    """ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í‚¤ ì°¾ê¸°"""
    if norm in pool:
        return norm
    # ë¶€ë¶„ ì¼ì¹˜: normì´ í‚¤ì— í¬í•¨ë˜ê±°ë‚˜ í‚¤ê°€ normì— í¬í•¨
    candidates = []
    for key in pool:
        if norm in key or key in norm:
            overlap = len(set(norm.split()) & set(key.split()))
            total   = max(len(norm.split()), len(key.split()))
            score   = overlap / total if total else 0
            if score >= threshold:
                candidates.append((score, key))
    if candidates:
        return max(candidates)[1]
    return None


def write_csv(rows: list[dict]):
    fieldnames = [
        "name_kr", "name_us", "name_jp",
        "kr_local", "kr_chat",
        "us_local", "us_chat",
        "jp_local", "jp_chat",
        "kr_url", "us_url", "jp_url",
    ]
    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow({k: row.get(k, "") for k in fieldnames})
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}  (ì´ {len(rows)}ê°œ ê²Œì„)")


# â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print("=" * 55)
    print("  ë‹Œí…ë„ ìŠ¤ìœ„ì¹˜ 2 GameShare ë‹¤ì§€ì—­ ìŠ¤í¬ë ˆì´í¼")
    print("  Nintendo Switch 2 GameShare Multi-Region Scraper")
    print("=" * 55)

    kr_results = scrape_kr()

    with sync_playwright() as pw:
        us_results = scrape_us(pw)
        jp_results = scrape_jp(pw)

    print("\nâ•â•â•â•â•â•â•â•â•â• ê²°ê³¼ ë³‘í•© & CSV ì €ì¥ â•â•â•â•â•â•â•â•â•â•")
    merged = merge_results(kr_results, us_results, jp_results)
    write_csv(merged)

    # ìš”ì•½ ì¶œë ¥
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ ì§€ì—­ë³„ GameShare ê²Œì„ ìˆ˜                â”‚")
    print(f"â”‚  ğŸ‡°ğŸ‡· í•œêµ­: {len(kr_results):3d}ê°œ                        â”‚")
    print(f"â”‚  ğŸ‡ºğŸ‡¸ ë¯¸êµ­: {len(us_results):3d}ê°œ                        â”‚")
    print(f"â”‚  ğŸ‡¯ğŸ‡µ ì¼ë³¸: {len(jp_results):3d}ê°œ                        â”‚")
    print(f"â”‚  ì „ì²´(ë³‘í•©): {len(merged):3d}ê°œ                     â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")


if __name__ == "__main__":
    main()
